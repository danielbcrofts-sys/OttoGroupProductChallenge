---
title: "Otto Group Product Challenge"
author: "Daniel Crofts"
format: html
editor: visual
---

## Data and Libraries

```{r}
library(tidyverse)
library(tidymodels)
library(xgboost)
library(vip)
library(glmnet)
library(janitor)
library(corrplot)
library(ranger)
library(data.table)
library(caret)
library(nnet)


set.seed(2025)

dat_train <- read_csv("train.csv")
dat_test <- read_csv("test.csv")
sample_submission <- read_csv("sampleSubmission.csv")

# glimpse(dat_train)
# glimpse(dat_test)



```

## Log Reg Model

```{r}
# Train multinomial logistic regression
lr_fit <- multinom(target ~ ., data = train_clean)

# Predict class probabilities for the test set
lr_preds <- predict(lr_fit, newdata = test_x_clean, type = "prob")

# Build Kaggle submission
submission_lr <- bind_cols(
  tibble(id = dat_test$id),
  as_tibble(lr_preds)
)

names(submission_lr) <- c("id", paste0("Class_", 1:9))

write_csv(submission_lr, "log_reg_submission.csv")

```
## Random Forest Model

```{r}
train <- dat_train %>% mutate(target = factor(target))

# NZV Removal
nzv_cols <- caret::nearZeroVar(train %>% select(-id, -target))
train_x_clean <- train %>% select(-id, -target) %>% select(-all_of(nzv_cols))
train_clean <- train_x_clean %>% mutate(target = train$target)

test_x_clean <- dat_test %>% 
  select(-id) %>% 
  select(-all_of(nzv_cols))

# SIMPLE recipe (no normalization)
rf_recipe <- recipe(target ~ ., data = train_clean)

# Random Forest Model
rf_model <- rand_forest(
  mtry  = 20,
  min_n = 5,
  trees = 1000
) %>%
  set_engine("ranger", importance = "impurity",
             num.threads = parallel::detectCores()) %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_model)

# ---- FAST: Fit ONCE on FULL TRAINING DATA ----
final_rf_fit <- fit(
  rf_wf,
  data = train_clean
)

# ---- Predict probabilities on test ----
rf_preds <- predict(
  final_rf_fit,
  new_data = test_x_clean,
  type = "prob"
)

# ---- Build submission ----
rf_submission <- bind_cols(
  tibble(id = dat_test$id),
  rf_preds
)

names(rf_submission) <- c("id", paste0("Class_", 1:9))

write_csv(rf_submission, "rf_submission.csv")


```

## XG Boost Model

```{r}
train <- dat_train %>% mutate(target = factor(target))

# Remove near-zero variance columns
nzv_cols <- caret::nearZeroVar(train %>% select(-id, -target))

train_x_clean <- train %>%
  select(-id, -target) %>%
  select(-all_of(nzv_cols))

test_x_clean <- dat_test %>%
  select(-id) %>%
  select(-all_of(nzv_cols))

# Reattach target
train_clean <- train_x_clean %>%
  mutate(target = train$target)

# Train/test split + CV folds
otto_split <- initial_split(train_clean, prop = 0.8, strata = target)
otto_train <- training(otto_split)
otto_test  <- testing(otto_split)

otto_folds <- vfold_cv(otto_train, v = 5, strata = target)

#==============================================
# 2. XGBoost Recipe (this replaces otto_recipe)
#==============================================

xgb_recipe <- recipe(target ~ ., data = otto_train) %>%
  step_normalize(all_predictors()) %>%
  step_zv(all_predictors())

#==============================================
# 3. XGBoost Model Specification
#==============================================

xgb_model <- boost_tree(
  trees = 1000,
  learn_rate = 0.05,
  tree_depth = 6,
  mtry = 20,
  min_n = 5,
  loss_reduction = 0,
  sample_size = 1
) %>%
  set_engine("xgboost", nthread = parallel::detectCores()) %>%
  set_mode("classification")

# Workflow (uses the correct recipe!)
xgb_wf <- workflow() %>%
  add_recipe(xgb_recipe) %>%
  add_model(xgb_model)

#==============================================
# 4. Cross-Validation Performance
#==============================================

xgb_res <- fit_resamples(
  xgb_wf,
  otto_folds,
  metrics = metric_set(accuracy, mn_log_loss),
  control = control_resamples(save_pred = TRUE)
)

collect_metrics(xgb_res)

#==============================================
# 5. Fit Final Model on ALL Training Data
#==============================================

final_xgb_fit <- fit(
  xgb_wf,
  data = train_clean
)

#==============================================
# 6. Predict Probabilities for Kaggle Submission
#==============================================

xgb_preds <- predict(
  final_xgb_fit,
  new_data = test_x_clean,
  type = "prob"
)

xgb_submission <- bind_cols(
  tibble(id = dat_test$id),
  xgb_preds
)

names(xgb_submission) <- c("id", paste0("Class_", 1:9))

write_csv(xgb_submission, "xgb_submission.csv")

```

